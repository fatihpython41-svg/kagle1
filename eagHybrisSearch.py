{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14840478,"sourceType":"datasetVersion","datasetId":9491736}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport fitz  # PyMuPDF\nfrom collections import Counter\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:10.983805Z","iopub.execute_input":"2026-02-14T16:28:10.984588Z","iopub.status.idle":"2026-02-14T16:28:11.001503Z","shell.execute_reply.started":"2026-02-14T16:28:10.984537Z","shell.execute_reply":"2026-02-14T16:28:11.000463Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/datasets/fatihpython/prometheusfa/Prometheus.pdf\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"pdf_path = \"/kaggle/input/datasets/fatihpython/prometheusfa/Prometheus.pdf\"\n\ndoc = fitz.open(pdf_path)\n\nprint(\"=\" * 60)\nprint(\"PDF BASIC INFORMATION\")\nprint(\"=\" * 60)\n\nprint(\"Total pages:\", doc.page_count)\nprint(\"Is encrypted:\", doc.is_encrypted)\nprint(\"Metadata:\")\nfor k, v in doc.metadata.items():\n    print(f\"  {k}: {v}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"TEXT ANALYSIS\")\nprint(\"=\" * 60)\n\ntotal_chars = 0\npage_lengths = []\nsample_text = \"\"\n\nfor i in range(doc.page_count):\n    page = doc.load_page(i)\n    text = page.get_text(\"text\")\n    char_count = len(text)\n    total_chars += char_count\n    page_lengths.append(char_count)\n\n    if i == 0:\n        sample_text = text[:1500]\n\nprint(\"Total characters in document:\", total_chars)\nprint(\"Average characters per page:\", total_chars // doc.page_count)\nprint(\"Empty pages:\", sum(1 for x in page_lengths if x < 50))\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"FIRST PAGE SAMPLE TEXT\")\nprint(\"=\" * 60)\nprint(sample_text)\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"STRUCTURE CHECK (HEADINGS HEURISTIC)\")\nprint(\"=\" * 60)\n\nheadings = []\nfor i in range(doc.page_count):\n    page = doc.load_page(i)\n    blocks = page.get_text(\"dict\")[\"blocks\"]\n    for b in blocks:\n        if b[\"type\"] == 0:\n            for line in b[\"lines\"]:\n                if len(line[\"spans\"]) == 1:\n                    span = line[\"spans\"][0]\n                    if span[\"size\"] >= 14 and span[\"text\"].strip():\n                        headings.append(span[\"text\"].strip())\n\nprint(\"Detected headings (sample):\")\nfor h in headings[:15]:\n    print(\"-\", h)\n\ndoc.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:11.003653Z","iopub.execute_input":"2026-02-14T16:28:11.004216Z","iopub.status.idle":"2026-02-14T16:28:14.462959Z","shell.execute_reply.started":"2026-02-14T16:28:11.004171Z","shell.execute_reply":"2026-02-14T16:28:14.461807Z"}},"outputs":[{"name":"stdout","text":"============================================================\nPDF BASIC INFORMATION\n============================================================\nTotal pages: 34\nIs encrypted: False\nMetadata:\n  format: PDF 1.5\n  title: Prometheus at Edinburgh\n  author:     Robert Currie, Edinburgh University   0-.33cm/ -.33cm1\n  subject: \n  keywords: \n  creator: LaTeX with Beamer class\n  producer: pdfTeX-1.40.20\n  creationDate: D:20190830081203+01'00'\n  modDate: D:20190830081203+01'00'\n  trapped: \n  encryption: None\n\n============================================================\nTEXT ANALYSIS\n============================================================\nTotal characters in document: 14123\nAverage characters per page: 415\nEmpty pages: 0\n\n============================================================\nFIRST PAGE SAMPLE TEXT\n============================================================\nWhat\nHow\nUsing monitoring tools\nEnd\nDeploying Prometheus at\nEdinburgh\naka: Making my life as a sysadmin easier.\nPrometheus at Edinburgh\n1 / 32\nRobert Currie, Edinburgh University\n\n\n============================================================\nSTRUCTURE CHECK (HEADINGS HEURISTIC)\n============================================================\nDetected headings (sample):\n- Edinburgh\n- Installing\n- Summary\n- Conclusion\n- BACKUPS\n- Security\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"import fitz  # PyMuPDF\nimport re\n\nPDF_PATH = \"/kaggle/input/datasets/fatihpython/prometheusfa/Prometheus.pdf\"\n\ndef clean_text(text: str) -> str:\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.replace('\\x00', '')\n    return text.strip()\n\ndoc = fitz.open(PDF_PATH)\n\ndocuments = []\ncurrent_section = None\n\nfor page_num in range(doc.page_count):\n    page = doc.load_page(page_num)\n    text = page.get_text(\"text\")\n\n    if not text.strip():\n        continue\n\n    text = clean_text(text)\n\n    # --- Simple heading detection (for metadata)\n    lines = text.split(\". \")\n    for line in lines[:3]:\n        if line.isupper() or len(line.split()) <= 4:\n            current_section = line.strip()\n\n    documents.append({\n        \"id\": f\"page_{page_num + 1}\",\n        \"text\": text,\n        \"metadata\": {\n            \"page\": page_num + 1,\n            \"title\": doc.metadata.get(\"title\", \"Unknown\"),\n            \"section\": current_section\n        }\n    })\n\ndoc.close()\n\nprint(f\"Total chunks created: {len(documents)}\")\n\n# Preview one chunk\ndocuments[5]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:14.464055Z","iopub.execute_input":"2026-02-14T16:28:14.464325Z","iopub.status.idle":"2026-02-14T16:28:14.546322Z","shell.execute_reply.started":"2026-02-14T16:28:14.464298Z","shell.execute_reply":"2026-02-14T16:28:14.545336Z"}},"outputs":[{"name":"stdout","text":"Total chunks created: 34\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'id': 'page_6',\n 'text': 'What How Using monitoring tools End Installing (Cont) For node exporter and Prometheus all you need to do is grab the binaries and run them. (and conﬁgure your ﬁrewall). The conﬁguration of all 3 parts is mostly handled by .yml ﬁles or point and click web interfaces. Bonus points: node exporter can run as an unprivileged user. A key advantage of these applications being written in go is that everything is statically shipped with the executable. No 3rd party libraries, no yum install. But this means no .service ﬁle or system integration. Prometheus at Edinburgh 6 / 32 Robert Currie, Edinburgh University',\n 'metadata': {'page': 6,\n  'title': 'Prometheus at Edinburgh',\n  'section': '(and conﬁgure your ﬁrewall)'}}"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport faiss\nimport numpy as np\n\n# Prepare texts\ntexts = [doc[\"text\"] for doc in documents]\n\n# Load embedding model\nembedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# Create embeddings\nembeddings = embedding_model.encode(\n    texts,\n    convert_to_numpy=True,\n    show_progress_bar=True,\n    normalize_embeddings=True\n)\n\n# Build FAISS index (cosine similarity)\ndim = embeddings.shape[1]\nfaiss_index = faiss.IndexFlatIP(dim)\nfaiss_index.add(embeddings)\n\nprint(\"Dense index size:\", faiss_index.ntotal)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:14.547445Z","iopub.execute_input":"2026-02-14T16:28:14.547805Z","iopub.status.idle":"2026-02-14T16:28:18.383120Z","shell.execute_reply.started":"2026-02-14T16:28:14.547775Z","shell.execute_reply":"2026-02-14T16:28:18.382452Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cffc01a332c64bbaa2a7520578cbebc8"}},"metadata":{}},{"name":"stdout","text":"Dense index size: 34\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"from rank_bm25 import BM25Okapi\n\ntokenized_corpus = [\n    doc[\"text\"].lower().split()\n    for doc in documents\n]\n\nbm25 = BM25Okapi(tokenized_corpus)\n\nprint(\"BM25 index ready\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:18.385019Z","iopub.execute_input":"2026-02-14T16:28:18.385314Z","iopub.status.idle":"2026-02-14T16:28:18.392701Z","shell.execute_reply.started":"2026-02-14T16:28:18.385283Z","shell.execute_reply":"2026-02-14T16:28:18.391811Z"}},"outputs":[{"name":"stdout","text":"BM25 index ready\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def hybrid_search(query, top_k=5, alpha=0.6):\n    \"\"\"\n    alpha = weight for dense search\n    (1 - alpha) = weight for sparse search\n    \"\"\"\n\n    # ----- Sparse search (BM25)\n    tokenized_query = query.lower().split()\n    bm25_scores = bm25.get_scores(tokenized_query)\n\n    # ----- Dense search\n    query_embedding = embedding_model.encode(\n        query,\n        normalize_embeddings=True\n    ).reshape(1, -1)\n\n    dense_scores, dense_indices = faiss_index.search(query_embedding, len(documents))\n\n    dense_scores = dense_scores[0]\n    dense_indices = dense_indices[0]\n\n    # Normalize BM25 scores\n    bm25_scores = np.array(bm25_scores)\n    bm25_scores = bm25_scores / (bm25_scores.max() + 1e-8)\n\n    # Fuse scores\n    final_scores = {}\n\n    for rank, idx in enumerate(dense_indices):\n        final_scores[idx] = alpha * dense_scores[rank]\n\n    for idx, score in enumerate(bm25_scores):\n        final_scores[idx] = final_scores.get(idx, 0) + (1 - alpha) * score\n\n    # Sort results\n    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n\n    results = []\n    for idx, score in ranked[:top_k]:\n        results.append({\n            \"score\": float(score),\n            \"text\": documents[idx][\"text\"],\n            \"metadata\": documents[idx][\"metadata\"]\n        })\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:18.393975Z","iopub.execute_input":"2026-02-14T16:28:18.394354Z","iopub.status.idle":"2026-02-14T16:28:18.417494Z","shell.execute_reply.started":"2026-02-14T16:28:18.394321Z","shell.execute_reply":"2026-02-14T16:28:18.416216Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"results = hybrid_search(\n    query=\"How is Prometheus installed and deployed?\",\n    top_k=3,\n    alpha=0.6\n)\n\nfor r in results:\n    print(\"\\nPAGE:\", r[\"metadata\"][\"page\"])\n    print(\"SCORE:\", round(r[\"score\"], 3))\n    print(r[\"text\"][:300])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:18.420391Z","iopub.execute_input":"2026-02-14T16:28:18.420852Z","iopub.status.idle":"2026-02-14T16:28:18.464673Z","shell.execute_reply.started":"2026-02-14T16:28:18.420799Z","shell.execute_reply":"2026-02-14T16:28:18.463355Z"}},"outputs":[{"name":"stdout","text":"\nPAGE: 2\nSCORE: 0.789\nWhat How Using monitoring tools End What is Prometheus? Oﬃcially: Prometheus is an open-source systems monitoring and alerting toolkit with an active ecosystem. My take: Prometheus is an easy to use, centralised, stateful, pull-based modular monitoring system with several mature supporting projects.\n\nPAGE: 6\nSCORE: 0.759\nWhat How Using monitoring tools End Installing (Cont) For node exporter and Prometheus all you need to do is grab the binaries and run them. (and conﬁgure your ﬁrewall). The conﬁguration of all 3 parts is mostly handled by .yml ﬁles or point and click web interfaces. Bonus points: node exporter can \n\nPAGE: 5\nSCORE: 0.691\nWhat How Using monitoring tools End Installing What components are needed to monitor a site with prometheus? • node exporter: node-github This runs on the various machines being monitored collecting system data. • Prometheus: Prometheus-github This is the main tool which collects the monitoring data\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# ============================================\n# HYBRID RAG SYSTEM (LOCAL LLM, INTERACTIVE)\n# ============================================\n\nimport re\nimport fitz\nimport faiss\nimport numpy as np\n\nfrom sentence_transformers import SentenceTransformer\nfrom rank_bm25 import BM25Okapi\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# --------------------------------------------\n# CONFIG\n# --------------------------------------------\nPDF_PATH = \"/kaggle/input/datasets/fatihpython/prometheusfa/Prometheus.pdf\"\nEMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\nLLM_MODEL = \"google/flan-t5-base\"   # CPU-friendly\nTOP_K = 5\nALPHA = 0.6\nMAX_CONTEXT_CHARS = 2500\n\n# --------------------------------------------\n# TEXT CLEANING\n# --------------------------------------------\ndef clean_text(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    return text.strip()\n\n# --------------------------------------------\n# PDF INGESTION\n# --------------------------------------------\ndoc = fitz.open(PDF_PATH)\ndocuments = []\ncurrent_section = None\n\nfor page_num in range(doc.page_count):\n    page = doc.load_page(page_num)\n    text = clean_text(page.get_text(\"text\"))\n\n    if not text:\n        continue\n\n    for line in text.split(\". \")[:3]:\n        if line.isupper() or len(line.split()) <= 4:\n            current_section = line\n\n    documents.append({\n        \"id\": f\"page_{page_num + 1}\",\n        \"text\": text,\n        \"metadata\": {\n            \"page\": page_num + 1,\n            \"section\": current_section\n        }\n    })\n\ndoc.close()\n\nprint(f\"Loaded {len(documents)} chunks\")\n\n# --------------------------------------------\n# SPARSE INDEX (BM25)\n# --------------------------------------------\ntokenized_corpus = [d[\"text\"].lower().split() for d in documents]\nbm25 = BM25Okapi(tokenized_corpus)\n\n# --------------------------------------------\n# DENSE INDEX (FAISS)\n# --------------------------------------------\nembedder = SentenceTransformer(EMBED_MODEL)\n\nembeddings = embedder.encode(\n    [d[\"text\"] for d in documents],\n    normalize_embeddings=True,\n    convert_to_numpy=True,\n    show_progress_bar=True\n)\n\ndim = embeddings.shape[1]\nfaiss_index = faiss.IndexFlatIP(dim)\nfaiss_index.add(embeddings)\n\n# --------------------------------------------\n# HYBRID SEARCH\n# --------------------------------------------\ndef hybrid_search(query, top_k=TOP_K, alpha=ALPHA):\n    bm25_scores = bm25.get_scores(query.lower().split())\n    bm25_scores = bm25_scores / (np.max(bm25_scores) + 1e-8)\n\n    q_emb = embedder.encode(query, normalize_embeddings=True).reshape(1, -1)\n    dense_scores, dense_ids = faiss_index.search(q_emb, len(documents))\n\n    dense_scores = dense_scores[0]\n    dense_ids = dense_ids[0]\n\n    final_scores = {}\n\n    for rank, idx in enumerate(dense_ids):\n        final_scores[idx] = alpha * dense_scores[rank]\n\n    for idx, score in enumerate(bm25_scores):\n        final_scores[idx] = final_scores.get(idx, 0) + (1 - alpha) * score\n\n    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n\n    return [\n        {\n            \"score\": float(score),\n            \"text\": documents[idx][\"text\"],\n            \"metadata\": documents[idx][\"metadata\"]\n        }\n        for idx, score in ranked[:top_k]\n    ]\n\n# --------------------------------------------\n# CONTEXT ASSEMBLY\n# --------------------------------------------\ndef build_context(results):\n    context = \"\"\n    for r in results:\n        block = f\"[Page {r['metadata']['page']}]\\n{r['text']}\\n\\n\"\n        if len(context) + len(block) > MAX_CONTEXT_CHARS:\n            break\n        context += block\n    return context.strip()\n\n# --------------------------------------------\n# LOCAL LLM (FLAN-T5)\n# --------------------------------------------\ntokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL)\n\ndef generate_answer(context, question):\n    prompt = f\"\"\"\nAnswer the question using ONLY the context below.\nIf the answer is not present, say:\n\"The document does not specify this.\"\n\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n\"\"\".strip()\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        temperature=0.0\n    )\n\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# --------------------------------------------\n# INTERACTIVE RAG LOOP\n# --------------------------------------------\nprint(\"\\nHybrid RAG ready.\")\nprint(\"Type your question or type 'exit' to quit.\\n\")\n\nwhile True:\n    question = input(\">> \")\n\n    if question.lower().strip() == \"exit\":\n        print(\"Exiting RAG system.\")\n        break\n\n    results = hybrid_search(question)\n    context = build_context(results)\n    answer = generate_answer(context, question)\n\n    print(\"\\nANSWER:\")\n    print(answer)\n    print(\"\\nSOURCES:\")\n    for r in results:\n        print(f\"- Page {r['metadata']['page']} (score={round(r['score'], 3)})\")\n    print(\"\\n\" + \"-\" * 60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T16:28:18.466004Z","iopub.execute_input":"2026-02-14T16:28:18.466375Z","iopub.status.idle":"2026-02-14T16:28:40.379023Z","shell.execute_reply.started":"2026-02-14T16:28:18.466335Z","shell.execute_reply":"2026-02-14T16:28:40.377733Z"}},"outputs":[{"name":"stdout","text":"Loaded 34 chunks\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f4c5e40e4440de8bc9629c309f07ea"}},"metadata":{}},{"name":"stdout","text":"\nHybrid RAG ready.\nType your question or type 'exit' to quit.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">>  what is prometheus?\n"},{"name":"stdout","text":"\nANSWER:\nan open-source systems monitoring and alerting toolkit with an active ecosystem\n\nSOURCES:\n- Page 2 (score=0.792)\n- Page 5 (score=0.651)\n- Page 3 (score=0.584)\n- Page 9 (score=0.491)\n- Page 29 (score=0.485)\n\n------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">>  exit\n"},{"name":"stdout","text":"Exiting RAG system.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\nprint('data was created')\n# Example: generate some data\ndata = {\n    'industry': ['Finance', 'Tech', 'AI'],\n    'layoff_count': [120, 200, 50],\n    'workforce_percentage': [5.5, 8.2, 3.0]\n}\n\ndf = pd.DataFrame(data)\n\n# Save as CSV to notebook output\n\ndf.to_csv('/kaggle/working/layoffs_processed.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T10:39:00.340820Z","iopub.execute_input":"2026-02-17T10:39:00.341655Z","iopub.status.idle":"2026-02-17T10:39:00.369820Z","shell.execute_reply.started":"2026-02-17T10:39:00.341614Z","shell.execute_reply":"2026-02-17T10:39:00.368754Z"}},"outputs":[{"name":"stdout","text":"data was created\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}